---
alwaysApply: false
---

# Remote Development Environment

## About LeRobot

**LeRobot** is Hugging Face's open-source robotics library that provides models, datasets, and tools for real-world robotics in PyTorch. The project focuses on imitation learning and reinforcement learning to enable robots to learn from human demonstrations.

### Project Context
- **Repository:** `/home/thor/lerobot`
- **Installation:** Editable install with `uv pip install -e ".[feetech]"`
- **Key Dependencies:**
  - PyTorch 2.9.1+cu130 (CUDA 13.0)
  - Feetech Servo SDK (for SO-ARM101 motors)
  - ffmpeg (for video encoding/decoding)
- **Purpose:** Training and deploying AI policies for the SO-ARM101 robotic arm
- **Workflow:** Data collection → Training → Policy deployment

### SO-ARM101 Robot
- Low-cost robotic arm designed by TheRobotStudio and Hugging Face
- Uses 6x STS3215 Feetech servos with magnetic encoders
- Supports teleoperation (leader arm) and autonomous control
- Compatible with LeRobot's imitation learning pipeline
- Documentation: [Seeed Studio Wiki](https://wiki.seeedstudio.com/lerobot_so100m/)

## System Configuration
- **Server:** Jetson AGX Thor (JetPack 7.0, Python 3.12, CUDA 13.0)
  - PyTorch: 2.9.1+cu130
  - CUDA: 13.0 (Active)
  - Device: NVIDIA Thor
  - Headless environment (no local display server)
  - No DISPLAY or WAYLAND_DISPLAY environment variables

- **Client:** MacBook M3
  - Connects to Jetson via SSH tunnel
  - Uses Cursor IDE with remote file server access
  - Files accessed from THOR using Cursor's remote file system

## Robot Configuration
- **Robot Type:** SO-ARM101 Follower Arm
- **USB Port:** `/dev/ttyACM0`
- **Robot ID:** `thor_follower_arm`
- **Motor Configuration:**
  - 6x STS3215 motors with IDs 1-6
  - shoulder_pan (ID 1)
  - shoulder_lift (ID 2)
  - elbow_flex (ID 3)
  - wrist_flex (ID 4)
  - wrist_roll (ID 5)
  - gripper (ID 6)
- **Calibration File:** `~/.cache/huggingface/lerobot/calibration/robots/so101_follower/thor_follower_arm.json`
- **Status:** Calibrated and connected
- **Note:** Some joints may be at calibration limits; use raw encoder positions to move to center if needed

## Camera Configuration
- **Camera Type:** Intel RealSense D435I
- **Serial Number:** `844212071286`
- **Firmware Version:** 5.17.0.10 (recovered from corrupted state)
- **USB Type:** 2.1 (shows as 2.1 but works fine)
- **Physical Port:** `/sys/devices/platform/bus@0/a80aa10000.usb/usb2/2-3/2-3.1/2-3.1:1.0/video4linux/video0`
- **Default Stream Profile:**
  - Stream Type: Color
  - Format: rgb8
  - Resolution: 1280x720
  - FPS: 30
- **SDK Installation:** Built from source in `/home/thor/librealsense`
  - C++ SDK: `librealsense2` (v2.54.2)
  - Python bindings: `pyrealsense2` (built from source, installed in virtual environment)
  - Build fix applied: Added `#include <cstdint>` for GCC 13 compatibility
- **Viewer:** Python-based viewer (`simple_realsense_viewer.py`) - official GUI not available in v2.54.2
- **Display Setup:** DISPLAY=:1 and XAUTHORITY configured in `~/.zshrc` and `~/.bashrc`
- **LeRobot Integration:** Compatible with `intelrealsense` camera type, tested and working
- **Status:** Installed, firmware recovered, tested, and ready for use

## Development Implications
- GUI applications can use Jetson's local monitor via DISPLAY=:1 (configured in .zshrc/.bashrc)
- RealSense viewer uses Python-based OpenCV viewer (official GUI not available in SDK v2.54.2)
- Visualization scripts should default to save mode (`--save 1`) or distant/server mode (`--mode distant`) for headless operation
- Display settings are in `.zshrc`/`.bashrc` - no separate scripts needed
- All file operations occur on the remote Jetson server, not the local MacBook
- Camera firmware recovery process documented in `.cursor/realsense_operational_notes.mdc`
